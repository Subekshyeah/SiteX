{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cc44ffd6",
      "metadata": {
        "id": "725d9998",
        "language": "markdown"
      },
      "source": [
        "# master.py\n",
        "\n",
        "Split into smaller cells for readability; converted from `backend/Data/master.py`.\n",
        "\n",
        "Note: This code references `__file__` when building paths. In a notebook, `__file__` may be undefined unless you execute these cells in a way that defines it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10fb48cc",
      "metadata": {},
      "source": [
        "## Imports & constants\n",
        "\n",
        "Describes the modules and global configuration (paths, category weights, defaults) shared by all later cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34df560d",
      "metadata": {
        "id": "c2ee0059",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Imports & constants\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Generate POI-influenced metrics for cafes and write a master CSV.\n",
        "\n",
        "\n",
        "\n",
        "Creates: backend/Data/master_cafes_metrics.csv\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "import math\n",
        "\n",
        "import argparse\n",
        "\n",
        "from typing import Optional, Tuple, List, Dict\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "DATA_DIR = os.path.join(os.path.dirname(__file__), \"CSV\")\n",
        "\n",
        "CAFE_FILE = os.path.join(DATA_DIR, \"cafes.csv\")\n",
        "\n",
        "MASTER_OUT = os.path.join(DATA_DIR, \"master_cafes_metrics.csv\")\n",
        "\n",
        "\n",
        "\n",
        "POI_FILES = {\n",
        "\n",
        "    \"banks\": os.path.join(DATA_DIR, \"banks.csv\"),\n",
        "\n",
        "    \"education\": os.path.join(DATA_DIR, \"education.csv\"),\n",
        "\n",
        "    \"health\": os.path.join(DATA_DIR, \"health.csv\"),\n",
        "\n",
        "    \"temples\": os.path.join(DATA_DIR, \"temples.csv\"),\n",
        "\n",
        "    \"other\": os.path.join(DATA_DIR, \"other.csv\"),\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Default category influence weights (can be tuned)\n",
        "\n",
        "CATEGORY_WEIGHTS = {\n",
        "\n",
        "    \"banks\": 0.6,\n",
        "\n",
        "    \"education\": 1.0,\n",
        "\n",
        "    \"health\": 0.9,\n",
        "\n",
        "    \"temples\": 0.8,\n",
        "\n",
        "    \"other\": 0.9,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Radius to use for master aggregation (meters) — set to 1.5 km\n",
        "\n",
        "MASTER_RADIUS_M = 1500\n",
        "\n",
        "DEFAULT_WEEKLY = 12 * 6\n",
        "\n",
        "# weight to apply to cafe-level properties when computing composite\n",
        "\n",
        "CAFE_PROPS_WEIGHT = 1.0\n",
        "\n",
        "# weight to apply to nearby cafes' combined properties\n",
        "\n",
        "CAFE_NEIGHBOR_WEIGHT = 1.0\n",
        "\n",
        "\n",
        "\n",
        "# Candidate names for lat/lon columns in CSVs\n",
        "\n",
        "LAT_COL_CANDS = [\"lat\", \"latitude\", \"y\", \"LAT\", \"Latitude\"]\n",
        "\n",
        "LON_COL_CANDS = [\"lon\", \"lng\", \"longitude\", \"x\", \"LON\", \"Longitude\"]\n",
        "\n",
        "\n",
        "\n",
        "# Candidate names for per-POI weight columns\n",
        "\n",
        "WEIGHT_COL_CANDS = [\"weight\", \"importance\", \"rating\", \"score\", \"pop\", \"count\", \"rank\"]\n",
        "\n",
        "RATING_COL_CANDS = [\"rating\", \"stars\"]\n",
        "\n",
        "REVIEWS_COL_CANDS = [\"reviewsCount\", \"reviews_count\", \"reviews\", \"reviewscount\", \"reviews_count\"]\n",
        "\n",
        "WEEKLY_HOURS_COL_CANDS = [\n",
        "\n",
        "    \"weekly_hours\",\n",
        "\n",
        "    \"weeklyHours\",\n",
        "\n",
        "    \"hours_per_week\",\n",
        "\n",
        "    \"weekly_open_hours\",\n",
        "\n",
        "    \"open_hours_week\",\n",
        "\n",
        "    \"weeklyopenhours\",\n",
        "\n",
        "    \"hours_week\",\n",
        "\n",
        "    \"weekly_opening_hours\",\n",
        "\n",
        "    \"opening_hours_week\",\n",
        "\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3a1ed6c",
      "metadata": {},
      "source": [
        "## Subcategory weight maps\n",
        "\n",
        "Explains the per-category weighting dictionaries used to bias different POI subtypes when computing influence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4d1be68",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Subcategory weight maps\n",
        "\n",
        "\n",
        "\n",
        "# Per-subcategory weights for education POIs. Edit values as needed.\n",
        "\n",
        "EDUCATION_SUBCAT_WEIGHTS = {\n",
        "\n",
        "    \"College\": 1.0,\n",
        "\n",
        "    \"Government school\": 1.0,\n",
        "\n",
        "    \"Driving school\": 1.0,\n",
        "\n",
        "    \"University\": 1.0,\n",
        "\n",
        "    \"High school\": 1.0,\n",
        "\n",
        "    \"Higher secondary school\": 1.0,\n",
        "\n",
        "    \"International school\": 1.0,\n",
        "\n",
        "    \"Language school\": 1.0,\n",
        "\n",
        "\n",
        "\n",
        "    \"Library\": 0.7,\n",
        "\n",
        "    \"Middle school\": 0.7,\n",
        "\n",
        "    \"After school program\": 0.5,\n",
        "\n",
        "    \"Art school\": 0.7,\n",
        "\n",
        "    \"Drivers license training school\": 1.0,\n",
        "\n",
        "    \"Bartending school\": 0.8,\n",
        "\n",
        "    \"Boarding school\": 0.7,\n",
        "\n",
        "    \"Business school\": 0.8,\n",
        "\n",
        "    \"Children_s library\": 0.5,\n",
        "\n",
        "    \"Chinese language school\": 0.7,\n",
        "\n",
        "    \"Combined primary and secondary school\": 0.7,\n",
        "\n",
        "    \"Community college\": 0.9,\n",
        "\n",
        "    \"Computer training school\": 0.8,\n",
        "\n",
        "    \"Dance school\": 0.8,\n",
        "\n",
        "\n",
        "\n",
        "    \"Drawing lessons\": 0.5,\n",
        "\n",
        "    \"Education center\": 0.5,\n",
        "\n",
        "    \"Educational institution\": 0.5,\n",
        "\n",
        "    \"Elementary school\": 0.5,\n",
        "\n",
        "    \"English language school\": 0.5,\n",
        "\n",
        "    \"Farm school\": 0.5,\n",
        "\n",
        "    \"General education school\": 0.5,\n",
        "\n",
        "    \"German language school\": 0.5,\n",
        "\n",
        "    \"Montessori preschool\": 0.5,\n",
        "\n",
        "    \"Montessori school\": 0.5,\n",
        "\n",
        "    \"Music school\": 0.5,\n",
        "\n",
        "    \"Preschool\": 0.5,\n",
        "\n",
        "    \"Primary school\": 0.5,\n",
        "\n",
        "    \"Private educational institution\": 0.5,\n",
        "\n",
        "    \"School center\": 0.5,\n",
        "\n",
        "    \"School house\": 0.5,\n",
        "\n",
        "    \"School supply store\": 0.5,\n",
        "\n",
        "    \"Secondary school\": 0.5,\n",
        "\n",
        "    \"Special education school\": 0.5,\n",
        "\n",
        "    \"Taekwondo school\": 0.5,\n",
        "\n",
        "    \"Technical school\": 0.5,\n",
        "\n",
        "    \"Training center\": 0.5,\n",
        "\n",
        "    \"Vocational school\": 0.5,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "BANK_SUBCAT_WEIGHTS = {\n",
        "\n",
        "    \"Bank\": 1.0,\n",
        "\n",
        "    \"Cooperative bank\": 0.7,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "HEALTH_SUBCAT_WEIGHTS = {\n",
        "\n",
        "    \"Hospital\": 1.0,\n",
        "\n",
        "    \"General hospital\": 1.0,\n",
        "\n",
        "    \"Government hospital\": 1.0,\n",
        "\n",
        "    \"Dentist\": 1.0,\n",
        "\n",
        "    \"Community health center\": 1.0,\n",
        "\n",
        "    \"Animal hospital\": 1.0,\n",
        "\n",
        "    \"Pharmacy\": 1.0,\n",
        "\n",
        "    \"Ayurvedic clinic\": 1.0,\n",
        "\n",
        "    \"Acupuncture clinic\": 1.0,\n",
        "\n",
        "    \"Public library\": 1.0,\n",
        "\n",
        "    \"Software company\": 1.0,\n",
        "\n",
        "    \"Medical clinic\": 1.0,\n",
        "\n",
        "    \"Orthopedic clinic\": 1.0,\n",
        "\n",
        "    \"Dental clinic\": 1.0,\n",
        "\n",
        "\n",
        "\n",
        "    \"Blood bank\": 0.5,\n",
        "\n",
        "    \"Cancer treatment center\": 0.5,\n",
        "\n",
        "    \"Child health care center\": 0.5,\n",
        "\n",
        "    \"Faculty of pharmacy\": 0.5,\n",
        "\n",
        "    \"Health and beauty shop\": 0.5,\n",
        "\n",
        "    \"Health consultant\": 0.5,\n",
        "\n",
        "    \"Health food store\": 0.5,\n",
        "\n",
        "    \"Health insurance agency\": 0.5,\n",
        "\n",
        "    \"Home health care service\": 0.5,\n",
        "\n",
        "    \"Hospital department\": 0.5,\n",
        "\n",
        "    \"Hospital equipment and supplies\": 0.5,\n",
        "\n",
        "    \"Hospitality and tourism school\": 0.5,\n",
        "\n",
        "    \"Mental health service\": 0.5,\n",
        "\n",
        "    \"Naturopathic practitioner\": 0.5,\n",
        "\n",
        "    \"Occupational health service\": 0.5,\n",
        "\n",
        "    \"Pain management physician\": 0.5,\n",
        "\n",
        "    \"Physical therapy c\": 0.5,\n",
        "\n",
        "    \"Private hospital\": 0.5,\n",
        "\n",
        "    \"Savings bank\": 0.5,\n",
        "\n",
        "    \"Self service health station\": 0.5,\n",
        "\n",
        "    \"Ticket office\": 0.5,\n",
        "\n",
        "    \"Tour operator\": 0.5,\n",
        "\n",
        "    \"Traffic police station\": 0.5,\n",
        "\n",
        "    \"Travel agency\": 0.5,\n",
        "\n",
        "    \"Veterinarian\": 0.5,\n",
        "\n",
        "    \"Veterinary pharmacy\": 0.5,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "OTHER_SUBCAT_WEIGHTS = {\n",
        "\n",
        "    \"Federal government office\": 1.0,\n",
        "\n",
        "    \"District office\": 1.0,\n",
        "\n",
        "    \"Post office\": 1.0,\n",
        "\n",
        "    \"Political party office\": 1.0,\n",
        "\n",
        "    \"State government office\": 1.0,\n",
        "\n",
        "    \"Government economic program\": 1.0,\n",
        "\n",
        "    \"Local government office\": 1.0,\n",
        "\n",
        "    \"Memorial park\": 1.0,\n",
        "\n",
        "    \"Athletic park\": 1.0,\n",
        "\n",
        "    \"Boxing gym\": 1.0,\n",
        "\n",
        "    \"Garden\": 1.0,\n",
        "\n",
        "    \"Gym\": 1.0,\n",
        "\n",
        "    \"Government office\": 1.0,\n",
        "\n",
        "    \"Government\": 1.0,\n",
        "\n",
        "    \"Park _ ride\": 1.0,\n",
        "\n",
        "    \"Park\": 1.0,\n",
        "\n",
        "    \"Muay Thai boxing gym\": 1.0,\n",
        "\n",
        "    \"Corporate office\": 1.0,\n",
        "\n",
        "\n",
        "\n",
        "    \"Water park\": 0.6,\n",
        "\n",
        "    \"Banquet hall\": 0.6,\n",
        "\n",
        "    \"Adventure sports center\": 0.6,\n",
        "\n",
        "    \"Beauty salon\": 0.6,\n",
        "\n",
        "    \"Beauty school\": 0.6,\n",
        "\n",
        "    \"Business park\": 0.6,\n",
        "\n",
        "    \"City government office\": 0.6,\n",
        "\n",
        "    \"Community garden\": 0.6,\n",
        "\n",
        "    \"Financial institution\": 0.6,\n",
        "\n",
        "    \"Food bank\": 0.6,\n",
        "\n",
        "    \"Military school\": 0.6,\n",
        "\n",
        "    \"Mobile home park\": 0.6,\n",
        "\n",
        "    \"Office supply store\": 0.6,\n",
        "\n",
        "    \"Photography studio\": 0.6,\n",
        "\n",
        "    \"Plaza\": 0.6,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "TEMPLE_SUBCAT_WEIGHTS = {\n",
        "\n",
        "    \"Buddhist temple\": 1.0,\n",
        "\n",
        "    \"Hindu temple\": 1.0,\n",
        "\n",
        "    \"Tourist attraction\": 1.0,\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15a1e456",
      "metadata": {},
      "source": [
        "## Geo + CSV helpers\n",
        "\n",
        "Summarizes the utility functions for geographic distance calculations and detecting key columns in the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18d49614",
      "metadata": {
        "id": "f04966ff",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Geo + CSV helpers\n",
        "def haversine_m(lat1: float, lon1: float, lat2: np.ndarray, lon2: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Vectorized haversine distance (meters) from a single point (lat1,lon1)\n",
        "    to arrays lat2, lon2 (in degrees).\n",
        "    \"\"\"\n",
        "    R = 6371000.0  # earth radius in meters\n",
        "    lat1_rad = math.radians(lat1)\n",
        "    lon1_rad = math.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0) ** 2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "    return R * c\n",
        "def detect_latlon(df: pd.DataFrame) -> Optional[Tuple[str, str]]:\n",
        "    for latc in LAT_COL_CANDS:\n",
        "        for lonc in LON_COL_CANDS:\n",
        "            if latc in df.columns and lonc in df.columns:\n",
        "                return latc, lonc\n",
        "    # fallback: try common pairs\n",
        "    pairs = [(\"latitude\", \"longitude\"), (\"lat\", \"lon\"), (\"y\", \"x\")]\n",
        "    for (a, b) in pairs:\n",
        "        if a in df.columns and b in df.columns:\n",
        "            return a, b\n",
        "    return None\n",
        "def detect_weight_col(df: pd.DataFrame) -> Optional[str]:\n",
        "    # Prefer an explicit rank column if present (may be named 'rank' or contain 'rank')\n",
        "    for c in df.columns:\n",
        "        if \"rank\" == c.lower() or \"rank\" in c.lower():\n",
        "            return c\n",
        "    for c in WEIGHT_COL_CANDS:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "def detect_name_col(df: pd.DataFrame) -> Optional[str]:\n",
        "    # common name/title columns\n",
        "    for c in (\"name\", \"title\", \"place\", \"place_name\"):\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04dd1942",
      "metadata": {},
      "source": [
        "## POI weight computation\n",
        "\n",
        "Describes how each POI row gets a combined weight from rank/review data plus subcategory-specific factors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dee878a",
      "metadata": {
        "id": "33f95973",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# POI weight computation\n",
        "def compute_weights_and_annotate(poi: pd.DataFrame, name: str) -> Tuple[pd.DataFrame, np.ndarray, float]:\n",
        "    \"\"\"Compute per-row combined weight from rank/weight, rating, and reviews.\n",
        "    Returns (annotated_poi_df, weights_array, dynamic_category_weight).\n",
        "    \"\"\"\n",
        "    df = poi.copy()\n",
        "    wc = detect_weight_col(df)\n",
        "    name_col = detect_name_col(df)\n",
        "\n",
        "    # Assign per-subcategory weights based on category name\n",
        "    mapping = None\n",
        "    default_weight = 0.5\n",
        "\n",
        "    if name == \"education\":\n",
        "        mapping = EDUCATION_SUBCAT_WEIGHTS\n",
        "    elif name == \"banks\":\n",
        "        mapping = BANK_SUBCAT_WEIGHTS\n",
        "    elif name == \"health\":\n",
        "        mapping = HEALTH_SUBCAT_WEIGHTS\n",
        "    elif name == \"other\":\n",
        "        mapping = OTHER_SUBCAT_WEIGHTS\n",
        "    elif name == \"temples\":\n",
        "        mapping = TEMPLE_SUBCAT_WEIGHTS\n",
        "\n",
        "    if mapping is not None:\n",
        "        # detect a category-like column\n",
        "        cat_col = None\n",
        "        for c in (\"category\", \"type\", \"place_type\", \"amenity\", \"class\", \"categoryName\", \"main_category\"):\n",
        "            if c in df.columns:\n",
        "                cat_col = c\n",
        "                break\n",
        "        if cat_col is not None:\n",
        "            df[\"subcategory\"] = df[cat_col]\n",
        "            # Map weights; if subcategory not in mapping, default to 0.5\n",
        "            df[\"subcategory_weight\"] = df[cat_col].map(mapping).fillna(0.5).astype(float)\n",
        "        else:\n",
        "            df[\"subcategory_weight\"] = default_weight\n",
        "    else:\n",
        "        # Fallback if no mapping found or category not configured\n",
        "        df[\"subcategory_weight\"] = 1.0\n",
        "\n",
        "    # detect rating/reviews\n",
        "    rating_col = None\n",
        "    reviews_col = None\n",
        "    weekly_col = None\n",
        "    # use module-level DEFAULT_WEEKLY\n",
        "    for c in RATING_COL_CANDS:\n",
        "        if c in df.columns:\n",
        "            rating_col = c\n",
        "            break\n",
        "    for c in REVIEWS_COL_CANDS:\n",
        "        if c in df.columns:\n",
        "            reviews_col = c\n",
        "            break\n",
        "    for c in WEEKLY_HOURS_COL_CANDS:\n",
        "        if c in df.columns:\n",
        "            weekly_col = c\n",
        "            break\n",
        "\n",
        "    vals = pd.to_numeric(df[wc], errors=\"coerce\") if wc is not None else pd.Series([np.nan] * len(df))\n",
        "\n",
        "    # base score\n",
        "    if wc is not None and \"rank\" in wc.lower():\n",
        "        maxr = vals.max(skipna=True)\n",
        "        if pd.isna(maxr) or maxr <= 0:\n",
        "            maxr = 1.0\n",
        "        fill_val = float(maxr) + 1.0\n",
        "        ranks = vals.fillna(fill_val).replace(0.0, fill_val).astype(float)\n",
        "        inv = 1.0 / (ranks + 1e-9)\n",
        "        base = inv / float(inv.max()) if inv.max() > 0 else pd.Series(np.ones(len(ranks)), index=ranks.index)\n",
        "        df[\"filled_rank\"] = ranks\n",
        "        df[\"base_score\"] = base\n",
        "    elif wc is not None:\n",
        "        v = vals.fillna(0.0).astype(float)\n",
        "        base = v / float(v.max()) if v.max() > 0 else pd.Series(np.zeros(len(v)), index=v.index)\n",
        "        df[\"filled_value\"] = v\n",
        "        df[\"base_score\"] = base\n",
        "    else:\n",
        "        base = pd.Series(np.zeros(len(df)), index=df.index)\n",
        "        df[\"base_score\"] = base\n",
        "\n",
        "    # rating\n",
        "    if rating_col is not None:\n",
        "        r = pd.to_numeric(df[rating_col], errors=\"coerce\").fillna(0.0).astype(float)\n",
        "        df[\"rating_raw\"] = r\n",
        "        df[\"rating_norm\"] = (r / 5.0).clip(0.0, 1.0)\n",
        "    else:\n",
        "        df[\"rating_raw\"] = np.nan\n",
        "        df[\"rating_norm\"] = 0.0\n",
        "\n",
        "    # reviews\n",
        "    if reviews_col is not None:\n",
        "        rv = pd.to_numeric(df[reviews_col], errors=\"coerce\").fillna(0.0).astype(float)\n",
        "        df[\"reviews_raw\"] = rv\n",
        "        logv = np.log1p(rv)\n",
        "        maxlog = float(logv.max()) if logv.max() > 0 else 1.0\n",
        "        df[\"reviews_norm\"] = (logv / maxlog) if maxlog > 0 else 0.0\n",
        "    else:\n",
        "        df[\"reviews_raw\"] = np.nan\n",
        "        df[\"reviews_norm\"] = 0.0\n",
        "\n",
        "    # weekly hours\n",
        "    if weekly_col is not None:\n",
        "        wh = pd.to_numeric(df[weekly_col], errors=\"coerce\")\n",
        "        wh = wh.fillna(DEFAULT_WEEKLY).astype(float)\n",
        "        # treat 0 or implausibly large values (>115) as missing and set to DEFAULT_WEEKLY\n",
        "        mask_bad = (wh == 0) | (wh > 115)\n",
        "        if mask_bad.any():\n",
        "            wh.loc[mask_bad] = float(DEFAULT_WEEKLY)\n",
        "        df[\"weekly_hours_raw\"] = wh\n",
        "        # normalize against DEFAULT_WEEKLY (cap at 1.0)\n",
        "        df[\"weekly_hours_norm\"] = (wh / float(DEFAULT_WEEKLY)).clip(0.0, 1.0)\n",
        "    else:\n",
        "        # if not present, assume DEFAULT_WEEKLY for all rows (so normalized factor = 1.0)\n",
        "        df[\"weekly_hours_raw\"] = float(DEFAULT_WEEKLY)\n",
        "        df[\"weekly_hours_norm\"] = 1.0\n",
        "\n",
        "    # combine available components: base (+ subcategory for education) and reviews only\n",
        "    comps = [df[\"base_score\"].to_numpy()]\n",
        "    # include subcategory weight for education if present\n",
        "    if \"subcategory_weight\" in df.columns:\n",
        "        comps.append(df[\"subcategory_weight\"].to_numpy())\n",
        "    # include reviews influence only (do NOT include rating or weekly-hours)\n",
        "    if reviews_col is not None and (df[\"reviews_raw\"].fillna(0.0) > 0).any():\n",
        "        comps.append(df[\"reviews_norm\"].to_numpy())\n",
        "\n",
        "    stacked = np.vstack(comps)\n",
        "    combined = np.nanmean(stacked, axis=0)\n",
        "    df[\"combined_score\"] = combined\n",
        "    df[\"_computed_weight\"] = combined\n",
        "\n",
        "    dyn_cat_w = float(np.nanmean(combined)) if len(combined) > 0 else 1.0\n",
        "    return df, combined.astype(float), dyn_cat_w\n",
        "\n",
        "    \n",
        "\n",
        "    dyn_cat_w = float(np.nanmean(combined)) if len(combined) > 0 else 1.0\n",
        "    return df, combined.astype(float), dyn_cat_w\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d69194b7",
      "metadata": {},
      "source": [
        "## Per-cafe POI metrics\n",
        "\n",
        "Explains how each cafe aggregates nearby POIs into counts, weights, and distance features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9c0701e",
      "metadata": {
        "id": "b7f461d1",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Per-cafe POI metrics\n",
        "def compute_poi_metrics_for_cafes(\n",
        "    cafes: pd.DataFrame,\n",
        "    poi: pd.DataFrame,\n",
        "    poi_name: str,\n",
        "    category_weight: float,\n",
        "    radius_m: float = 1000.0,\n",
        ") -> pd.DataFrame:\n",
        "    latlon = detect_latlon(poi)\n",
        "    if latlon is None:\n",
        "        # no coordinates in POI, return zeros\n",
        "        cafes[f\"{poi_name}_count_1km\"] = 0\n",
        "        cafes[f\"{poi_name}_weight_1km\"] = 0.0\n",
        "        cafes[f\"{poi_name}_min_dist_m\"] = np.nan\n",
        "        return cafes\n",
        "\n",
        "    poi_lat_col, poi_lon_col = latlon\n",
        "    poi_lats = poi[poi_lat_col].astype(float).to_numpy()\n",
        "    poi_lons = poi[poi_lon_col].astype(float).to_numpy()\n",
        "\n",
        "    weight_col = detect_weight_col(poi)\n",
        "    # if a precomputed weight column exists (created by helper), use it\n",
        "    if \"_computed_weight\" in poi.columns:\n",
        "        poi_weights = pd.to_numeric(poi[\"_computed_weight\"], errors=\"coerce\").fillna(1.0).to_numpy().astype(float)\n",
        "    else:\n",
        "        # gather optional rating/review columns\n",
        "        def _detect_rating_col(df: pd.DataFrame) -> Optional[str]:\n",
        "            for c in RATING_COL_CANDS:\n",
        "                if c in df.columns:\n",
        "                    return c\n",
        "            return None\n",
        "\n",
        "        def _detect_reviews_col(df: pd.DataFrame) -> Optional[str]:\n",
        "            for c in REVIEWS_COL_CANDS:\n",
        "                if c in df.columns:\n",
        "                    return c\n",
        "            return None\n",
        "\n",
        "        rating_col = _detect_rating_col(poi)\n",
        "        reviews_col = _detect_reviews_col(poi)\n",
        "\n",
        "        if weight_col is not None:\n",
        "            raw_vals = pd.to_numeric(poi[weight_col], errors=\"coerce\")\n",
        "            # base score from rank-like column (lower is better)\n",
        "            if \"rank\" in weight_col.lower():\n",
        "                maxr = raw_vals.max(skipna=True)\n",
        "                if pd.isna(maxr) or maxr <= 0:\n",
        "                    maxr = 1.0\n",
        "                fill_val = float(maxr) + 1.0\n",
        "                ranks = raw_vals.fillna(fill_val).replace(0.0, fill_val).astype(float)\n",
        "                inv = 1.0 / (ranks + 1e-9)\n",
        "                base_norm = inv / float(inv.max()) if inv.max() > 0 else pd.Series(np.ones(len(ranks)), index=ranks.index)\n",
        "            else:\n",
        "                # treat column as higher-is-better; normalize by max\n",
        "                v = raw_vals.fillna(0.0).astype(float)\n",
        "                base_norm = v / float(v.max()) if v.max() > 0 else pd.Series(np.zeros(len(v)), index=v.index)\n",
        "\n",
        "            # optional rating and reviews influence\n",
        "            rating_vals = pd.to_numeric(poi[rating_col], errors=\"coerce\") if rating_col is not None else None\n",
        "            reviews_vals = pd.to_numeric(poi[reviews_col], errors=\"coerce\") if reviews_col is not None else None\n",
        "\n",
        "            # Use base/rank and (optionally) reviews only — do NOT use rating or weekly-hours\n",
        "            comps = [base_norm]\n",
        "            if reviews_vals is not None:\n",
        "                rv = reviews_vals.fillna(0.0).astype(float)\n",
        "                maskr = rv > 0\n",
        "                if maskr.any():\n",
        "                    # log-scale normalize by max\n",
        "                    norm_rev = pd.Series(0.0, index=rv.index)\n",
        "                    maxlog = float(np.log1p(rv[maskr]).max())\n",
        "                    if maxlog > 0:\n",
        "                        norm_rev[maskr] = np.log1p(rv[maskr]) / maxlog\n",
        "                    comps.append(norm_rev)\n",
        "\n",
        "            # final per-POI weight is mean of available components\n",
        "            stacked = np.vstack([c.to_numpy() for c in comps])\n",
        "            poi_weights = np.nanmean(stacked, axis=0).astype(float)\n",
        "        else:\n",
        "            # no explicit weight column: use reviews only (do NOT use rating or weekly-hours)\n",
        "            reviews_vals = pd.to_numeric(poi[reviews_col], errors=\"coerce\") if reviews_col is not None else None\n",
        "            if reviews_vals is not None:\n",
        "                rv = reviews_vals.fillna(0.0).astype(float)\n",
        "                maskr = rv > 0\n",
        "                norm_rev = pd.Series(0.0, index=rv.index)\n",
        "                if maskr.any():\n",
        "                    maxlog = float(np.log1p(rv[maskr]).max())\n",
        "                    if maxlog > 0:\n",
        "                        norm_rev[maskr] = np.log1p(rv[maskr]) / maxlog\n",
        "                    poi_weights = norm_rev.to_numpy().astype(float)\n",
        "                else:\n",
        "                    poi_weights = np.ones_like(poi_lats, dtype=float)\n",
        "            else:\n",
        "                poi_weights = np.ones_like(poi_lats, dtype=float)\n",
        "\n",
        "    # Prepare result columns (use suffix based on radius)\n",
        "    try:\n",
        "        suffix = f\"_{int(radius_m/1000)}km\"\n",
        "    except Exception:\n",
        "        suffix = \"_1km\"\n",
        "    counts = []\n",
        "    weight_sums = []\n",
        "    min_dists = []\n",
        "\n",
        "    # detect cafe lat/lon columns\n",
        "    cafe_latlon = detect_latlon(cafes)\n",
        "    if cafe_latlon is None:\n",
        "        raise ValueError(\"Could not detect lat/lon in cafes CSV\")\n",
        "\n",
        "    cafe_lat_col, cafe_lon_col = cafe_latlon\n",
        "    cafe_lats = cafes[cafe_lat_col].astype(float).to_numpy()\n",
        "    cafe_lons = cafes[cafe_lon_col].astype(float).to_numpy()\n",
        "\n",
        "    # Iterate cafes and compute vectorized distances\n",
        "    for i in range(len(cafes)):\n",
        "        lat = cafe_lats[i]\n",
        "        lon = cafe_lons[i]\n",
        "        dists = haversine_m(lat, lon, poi_lats, poi_lons)  # meters\n",
        "        within_mask = dists <= radius_m\n",
        "        counts.append(int(np.count_nonzero(within_mask)))\n",
        "        if np.any(within_mask):\n",
        "            weight_sum = float(np.sum(poi_weights[within_mask]))\n",
        "            weight_sums.append(weight_sum)\n",
        "            min_dists.append(float(np.min(dists[within_mask])))\n",
        "        else:\n",
        "            weight_sums.append(0.0)\n",
        "            min_dists.append(float(np.nan))\n",
        "\n",
        "    cafes[f\"{poi_name}_count{suffix}\"] = counts\n",
        "    cafes[f\"{poi_name}_weight{suffix}\"] = weight_sums\n",
        "    cafes[f\"{poi_name}_min_dist_m\"] = min_dists\n",
        "\n",
        "    # Also store category weight so downstream composite score can use it\n",
        "    cafes[f\"{poi_name}_category_weight\"] = category_weight\n",
        "\n",
        "    return cafes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8867d02d",
      "metadata": {},
      "source": [
        "## Master dataset generation\n",
        "\n",
        "Outlines the orchestration that merges POI features into cafes, builds composite scores, and writes multiple CSV outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbb3e0e5",
      "metadata": {
        "id": "176c49c2",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Master dataset generation\n",
        "def generate_master_metrics(\n",
        "    cafe_file: str = CAFE_FILE,\n",
        "    poi_files: Dict[str, str] = POI_FILES,\n",
        "    category_weights: Dict[str, float] = CATEGORY_WEIGHTS,\n",
        "    out_file: str = MASTER_OUT,\n",
        "):\n",
        "    cafes = pd.read_csv(cafe_file)\n",
        "    if detect_latlon(cafes) is None:\n",
        "        raise ValueError(f\"Could not detect lat/lon columns in cafes file: {cafe_file}\")\n",
        "\n",
        "    # Ensure deterministic order\n",
        "    cafes = cafes.reset_index(drop=True)\n",
        "\n",
        "    # For each POI dataset, compute metrics and merge into cafes\n",
        "    # Use configured radius for master aggregation unless overridden\n",
        "    master_radius = MASTER_RADIUS_M\n",
        "    for name, path in poi_files.items():\n",
        "        if not os.path.exists(path):\n",
        "            cafes[f\"{name}_count_1km\"] = 0\n",
        "            cafes[f\"{name}_weight_1km\"] = 0.0\n",
        "            cafes[f\"{name}_min_dist_m\"] = np.nan\n",
        "            cafes[f\"{name}_category_weight\"] = category_weights.get(name, 1.0)\n",
        "            continue\n",
        "        poi = pd.read_csv(path)\n",
        "        # compute per-POI weights, annotate POI df, save per-category CSV\n",
        "        try:\n",
        "            annotated_poi, weights, dyn_cat_w = compute_weights_and_annotate(poi, name)\n",
        "        except Exception:\n",
        "            annotated_poi = poi.copy()\n",
        "            weights = np.ones(len(poi), dtype=float)\n",
        "            dyn_cat_w = float(category_weights.get(name, 1.0))\n",
        "        # save annotated POI data\n",
        "        out_path = os.path.join(DATA_DIR, f\"{name}_all_data.csv\")\n",
        "        try:\n",
        "            annotated_poi.to_csv(out_path, index=False)\n",
        "            print(f\"Wrote annotated POI data for {name} to: {out_path}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        # also write a lightweight final CSV with only name, lat, lon, category (if available)\n",
        "        try:\n",
        "            final_cols = []\n",
        "            # detect name and lat/lon in annotated poi\n",
        "            poi_name_col = detect_name_col(annotated_poi)\n",
        "            latlon = detect_latlon(annotated_poi)\n",
        "            category_col = None\n",
        "            for c in (\"category\", \"type\", \"place_type\", \"amenity\", \"class\"):\n",
        "                if c in annotated_poi.columns:\n",
        "                    category_col = c\n",
        "                    break\n",
        "            if poi_name_col is not None:\n",
        "                final_cols.append(poi_name_col)\n",
        "            if latlon is not None:\n",
        "                final_cols.extend([latlon[0], latlon[1]])\n",
        "            if category_col is not None:\n",
        "                final_cols.append(category_col)\n",
        "            # include subcategory weight for education POIs\n",
        "            if name == \"education\" and \"subcategory_weight\" in annotated_poi.columns and \"subcategory_weight\" not in final_cols:\n",
        "                final_cols.append(\"subcategory_weight\")\n",
        "            # include computed weight/score if present\n",
        "            if \"_computed_weight\" in annotated_poi.columns:\n",
        "                final_cols.append(\"_computed_weight\")\n",
        "            elif \"combined_score\" in annotated_poi.columns:\n",
        "                final_cols.append(\"combined_score\")\n",
        "            # include original weight/rank column(s) if present so final CSV preserves rank info\n",
        "            detected_wc = detect_weight_col(annotated_poi)\n",
        "            if detected_wc is not None and detected_wc not in final_cols:\n",
        "                final_cols.append(detected_wc)\n",
        "            # include any explicit rank-like columns (e.g., 'rank', 'filled_rank')\n",
        "            for c in annotated_poi.columns:\n",
        "                try:\n",
        "                    if \"rank\" == c.lower() or \"rank\" in c.lower():\n",
        "                        if c not in final_cols:\n",
        "                            final_cols.append(c)\n",
        "                except Exception:\n",
        "                    continue\n",
        "            if final_cols:\n",
        "                # ensure uniqueness and preserve order\n",
        "                seen = set()\n",
        "                final_cols = [x for x in final_cols if not (x in seen or seen.add(x))]\n",
        "                final_df = annotated_poi.loc[:, final_cols].copy()\n",
        "                # normalize column name for final weight\n",
        "                if \"_computed_weight\" in final_df.columns:\n",
        "                    final_df = final_df.rename(columns={\"_computed_weight\": \"final_weight\"})\n",
        "                if \"combined_score\" in final_df.columns and \"final_weight\" not in final_df.columns:\n",
        "                    final_df = final_df.rename(columns={\"combined_score\": \"final_weight\"})\n",
        "                # ensure final output folder exists\n",
        "                final_dir = os.path.join(DATA_DIR, \"final\")\n",
        "                os.makedirs(final_dir, exist_ok=True)\n",
        "                final_out = os.path.join(final_dir, f\"{name}_final.csv\")\n",
        "                final_df.to_csv(final_out, index=False)\n",
        "                print(f\"Wrote final POI CSV for {name} to: {final_out}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        # override the category weight for this category so later scoring uses the dynamic value\n",
        "        category_weights[name] = dyn_cat_w\n",
        "        # pass annotated poi (which includes '_computed_weight') into metric computation\n",
        "        cafes = compute_poi_metrics_for_cafes(cafes, annotated_poi, name, dyn_cat_w, radius_m=master_radius)\n",
        "\n",
        "    # After processing all POI categories, build the composite score (using master radius)\n",
        "    suffix = f\"_{int(master_radius/1000)}km\"\n",
        "    score_components = []\n",
        "    for pname in poi_files.keys():\n",
        "        weight_col = f\"{pname}_weight{suffix}\"\n",
        "        cat_w = cafes.get(f\"{pname}_category_weight\", series_or_scalar(category_weights.get(pname, 1.0)))\n",
        "        max_val = cafes[weight_col].max() if weight_col in cafes.columns else 0.0\n",
        "        if pd.isna(max_val) or max_val == 0:\n",
        "            norm = np.zeros(len(cafes))\n",
        "        else:\n",
        "            norm = cafes[weight_col].astype(float) / float(max_val)\n",
        "        score_components.append(norm * float(category_weights.get(pname, 1.0)))\n",
        "\n",
        "    # Include cafe-level properties (rating, reviews, weekly_hours, rank) as an additional component\n",
        "    cafe_prop_components = []\n",
        "    # rating: intentionally ignored for cafe-level scoring (use star-distribution + review counts instead)\n",
        "    caf_rating_col = None\n",
        "    for c in RATING_COL_CANDS:\n",
        "        if c in cafes.columns:\n",
        "            caf_rating_col = c\n",
        "            break\n",
        "    # reviews\n",
        "    caf_reviews_col = None\n",
        "    for c in REVIEWS_COL_CANDS:\n",
        "        if c in cafes.columns:\n",
        "            caf_reviews_col = c\n",
        "            break\n",
        "    if caf_reviews_col is not None:\n",
        "        rv = pd.to_numeric(cafes[caf_reviews_col], errors=\"coerce\").fillna(0.0).astype(float)\n",
        "        logv = np.log1p(rv)\n",
        "        maxlog = float(logv.max()) if logv.max() > 0 else 1.0\n",
        "        cafe_prop_components.append((logv / maxlog).to_numpy())\n",
        "    # weekly hours\n",
        "    caf_weekly_col = None\n",
        "    for c in WEEKLY_HOURS_COL_CANDS:\n",
        "        if c in cafes.columns:\n",
        "            caf_weekly_col = c\n",
        "            break\n",
        "    if caf_weekly_col is not None:\n",
        "        wh = pd.to_numeric(cafes[caf_weekly_col], errors=\"coerce\").fillna(DEFAULT_WEEKLY).astype(float)\n",
        "        cafe_prop_components.append((wh / float(DEFAULT_WEEKLY)).clip(0.0, 1.0).to_numpy())\n",
        "    # do not use any cafe-level `rank` column in scoring\n",
        "\n",
        "    if cafe_prop_components:\n",
        "        cafe_props = np.nanmean(np.vstack(cafe_prop_components), axis=0)\n",
        "        score_components.append(cafe_props * float(CAFE_PROPS_WEIGHT))\n",
        "\n",
        "    # Compute sum of nearby cafes' property-weights for each cafe (exclude itself)\n",
        "    try:\n",
        "        cafe_latlon = detect_latlon(cafes)\n",
        "        if cafe_latlon is not None:\n",
        "            clat_col, clon_col = cafe_latlon\n",
        "            cafe_lats = cafes[clat_col].astype(float).to_numpy()\n",
        "            cafe_lons = cafes[clon_col].astype(float).to_numpy()\n",
        "            # ensure cafe_props exists\n",
        "            if 'cafe_props' not in locals():\n",
        "                cafe_props = np.zeros(len(cafes), dtype=float)\n",
        "            nearby_vals = []\n",
        "            for i in range(len(cafes)):\n",
        "                dists = haversine_m(cafe_lats[i], cafe_lons[i], cafe_lats, cafe_lons)\n",
        "                mask = (dists <= master_radius)\n",
        "                mask[i] = False\n",
        "                nearby_vals.append(float(np.sum(cafe_props[mask])))\n",
        "            nearby_arr = np.array(nearby_vals, dtype=float)\n",
        "            # save raw nearby weight column\n",
        "            cafes[f\"cafes_nearby_weight{suffix}\"] = nearby_arr\n",
        "            # normalize nearby_arr to 0..1 for inclusion\n",
        "            maxn = float(nearby_arr.max()) if nearby_arr.size > 0 else 0.0\n",
        "            if maxn > 0:\n",
        "                nearby_norm = nearby_arr / maxn\n",
        "            else:\n",
        "                nearby_norm = np.zeros_like(nearby_arr)\n",
        "            score_components.append(nearby_norm * float(CAFE_NEIGHBOR_WEIGHT))\n",
        "            # also compute count of other cafes within 1km\n",
        "            try:\n",
        "                count_radius = 1000.0\n",
        "                cafe_counts_1km = []\n",
        "                for i in range(len(cafes)):\n",
        "                    dists = haversine_m(cafe_lats[i], cafe_lons[i], cafe_lats, cafe_lons)\n",
        "                    maskc = (dists <= count_radius)\n",
        "                    maskc[i] = False\n",
        "                    cafe_counts_1km.append(int(np.count_nonzero(maskc)))\n",
        "                cafes[f\"cafes_count_1km\"] = cafe_counts_1km\n",
        "            except Exception:\n",
        "                cafes[f\"cafes_count_1km\"] = 0\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if score_components:\n",
        "        composite = np.sum(np.vstack(score_components), axis=0)\n",
        "    else:\n",
        "        composite = np.zeros(len(cafes))\n",
        "\n",
        "    cafes[\"poi_composite_score\"] = composite\n",
        "\n",
        "    # Ensure `cafe_props` exists and save individual cafe-level score\n",
        "    if 'cafe_props' not in locals():\n",
        "        cafe_props = np.zeros(len(cafes), dtype=float)\n",
        "    cafes[\"cafe_individual_score\"] = cafe_props\n",
        "\n",
        "    # Normalize individual score to create a cafe-level weight (0..1)\n",
        "    try:\n",
        "        max_ind = float(np.nanmax(cafe_props)) if len(cafe_props) > 0 else 0.0\n",
        "    except Exception:\n",
        "        max_ind = 0.0\n",
        "    if max_ind > 0:\n",
        "        cafes[\"cafe_weight\"] = cafe_props / max_ind\n",
        "    else:\n",
        "        cafes[\"cafe_weight\"] = 0.0\n",
        "\n",
        "    # Save master\n",
        "    cafes.to_csv(out_file, index=False)\n",
        "    print(f\"Master dataset written to: {out_file}\")\n",
        "\n",
        "    # Also write a fuller cafe_final CSV with per-cafe individual score, weight and POI counts\n",
        "    try:\n",
        "        final_dir = os.path.join(DATA_DIR, \"final\")\n",
        "        os.makedirs(final_dir, exist_ok=True)\n",
        "        # choose a set of useful columns for cafe_final\n",
        "        cols = []\n",
        "        title_col = detect_name_col(cafes)\n",
        "        if title_col is not None:\n",
        "            cols.append(title_col)\n",
        "        cafe_latlon = detect_latlon(cafes)\n",
        "        if cafe_latlon is not None:\n",
        "            cols.extend([cafe_latlon[0], cafe_latlon[1]])\n",
        "        # basic cafe fields (exclude raw `rating` and raw `rank` columns from outputs and scoring)\n",
        "        for c in (caf_reviews_col, caf_weekly_col):\n",
        "            if c is not None and c in cafes.columns:\n",
        "                cols.append(c)\n",
        "        # include individual score and weight\n",
        "        cols.append(\"cafe_individual_score\")\n",
        "        cols.append(\"cafe_weight\")\n",
        "        # include POI counts and weights for visibility\n",
        "        for pname in poi_files.keys():\n",
        "            count_col = f\"{pname}_count{suffix}\"\n",
        "            wcol = f\"{pname}_weight{suffix}\"\n",
        "            if count_col in cafes.columns:\n",
        "                cols.append(count_col)\n",
        "            if wcol in cafes.columns:\n",
        "                cols.append(wcol)\n",
        "        # ensure uniqueness and fallback to full frame if empty\n",
        "        seen = set()\n",
        "        final_cols = [x for x in cols if not (x in seen or seen.add(x))]\n",
        "        if not final_cols:\n",
        "            final_df = cafes.copy()\n",
        "        else:\n",
        "            final_df = cafes.loc[:, final_cols].copy()\n",
        "        # add per-category ranks (higher weight/count gets rank 1)\n",
        "        for pname in poi_files.keys():\n",
        "            wcol = f\"{pname}_weight{suffix}\"\n",
        "            count_col = f\"{pname}_count{suffix}\"\n",
        "            rank_col = f\"{pname}_rank\"\n",
        "            if wcol in final_df.columns:\n",
        "                try:\n",
        "                    final_df[rank_col] = final_df[wcol].rank(method=\"min\", ascending=False)\n",
        "                except Exception:\n",
        "                    final_df[rank_col] = pd.NA\n",
        "            elif count_col in final_df.columns:\n",
        "                try:\n",
        "                    final_df[rank_col] = final_df[count_col].rank(method=\"min\", ascending=False)\n",
        "                except Exception:\n",
        "                    final_df[rank_col] = pd.NA\n",
        "        final_out = os.path.join(final_dir, \"cafe_final.csv\")\n",
        "        final_df.to_csv(final_out, index=False)\n",
        "        print(f\"Wrote cafe final CSV to: {final_out}\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Also write a minimal master with only title (or name) and assigned scores\n",
        "    try:\n",
        "        title_col = detect_name_col(cafes) or \"title\"\n",
        "        minimal_cols = []\n",
        "        if title_col in cafes.columns:\n",
        "            minimal_cols.append(title_col)\n",
        "        # include cafe lat/lon if available\n",
        "        cafe_latlon = detect_latlon(cafes)\n",
        "        if cafe_latlon is not None:\n",
        "            lat_col, lon_col = cafe_latlon\n",
        "            # insert lat/lon after title\n",
        "            minimal_cols.append(lat_col)\n",
        "            minimal_cols.append(lon_col)\n",
        "        # include cafe category if present\n",
        "        category_col = None\n",
        "        for c in (\"category\", \"categoryName\", \"main_category\"):\n",
        "            if c in cafes.columns:\n",
        "                category_col = c\n",
        "                break\n",
        "        if category_col is not None:\n",
        "            minimal_cols.append(category_col)\n",
        "        # include counts and weights per POI category\n",
        "        for pname in poi_files.keys():\n",
        "            count_col = f\"{pname}_count{suffix}\"\n",
        "            wcol = f\"{pname}_weight{suffix}\"\n",
        "            if count_col in cafes.columns:\n",
        "                minimal_cols.append(count_col)\n",
        "            if wcol in cafes.columns:\n",
        "                minimal_cols.append(wcol)\n",
        "        # include computed cafe weight and composite score\n",
        "        minimal_cols.append(\"cafe_weight\")\n",
        "        minimal_cols.append(\"poi_composite_score\")\n",
        "        minimal_df = cafes.loc[:, minimal_cols].copy()\n",
        "        minimal_out_dir = os.path.join(DATA_DIR, \"final\")\n",
        "        os.makedirs(minimal_out_dir, exist_ok=True)\n",
        "        minimal_out = os.path.join(minimal_out_dir, \"master_cafes_minimal.csv\")\n",
        "        minimal_df.to_csv(minimal_out, index=False)\n",
        "        print(f\"Wrote minimal master CSV to: {minimal_out}\")\n",
        "    except Exception:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd8a3860",
      "metadata": {},
      "source": [
        "## CLI entrypoint\n",
        "\n",
        "Documents the command-line interface helpers so you know how to run the script version from a shell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c904d98",
      "metadata": {
        "id": "634e118d",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# CLI entrypoint\n",
        "def series_or_scalar(x):\n",
        "    # helper to allow retrieving category weight when column absent\n",
        "    return x\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Generate cafe POI metrics and master dataset.\")\n",
        "    parser.add_argument(\"--cafes\", default=CAFE_FILE, help=\"Path to cafes CSV\")\n",
        "    parser.add_argument(\"--out\", default=MASTER_OUT, help=\"Output master CSV path\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    generate_master_metrics(cafe_file=args.cafes, out_file=args.out)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
