{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cc44ffd6",
      "metadata": {
        "id": "725d9998",
        "language": "markdown"
      },
      "source": [
        "# master.py\n",
        "\n",
        "Split into smaller cells for readability; converted from `backend/Data/master.py`.\n",
        "\n",
        "Note: This code references `__file__` when building paths. In a notebook, `__file__` may be undefined unless you execute these cells in a way that defines it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10fb48cc",
      "metadata": {},
      "source": [
        "## Imports & constants\n",
        "\n",
        "Describes the modules and global configuration (paths, category weights, defaults) shared by all later cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "34df560d",
      "metadata": {
        "id": "c2ee0059",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Imports & constants\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "\"\"\"\n",
        "Generate POI-influenced metrics for cafes and write a master CSV.\n",
        "\n",
        "Creates: backend/Data/master_cafes_metrics.csv\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from typing import Optional, Tuple, List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def _resolve_data_dir() -> str:\n",
        "    anchor = (\n",
        "        Path(__file__).parent\n",
        "        if \"__file__\" in globals()\n",
        "        else Path.cwd()\n",
        "    ).resolve()\n",
        "    bases = {anchor, anchor.parent, anchor.parent.parent}\n",
        "    suffixes = [\n",
        "        Path(\"\"),\n",
        "        Path(\"CSV\"),\n",
        "        Path(\"CSV_Reference\"),\n",
        "        Path(\"Data\"),\n",
        "        Path(\"Data\") / \"CSV\",\n",
        "        Path(\"Data\") / \"CSV_Reference\",\n",
        "    ]\n",
        "    for base in bases:\n",
        "        for suffix in suffixes:\n",
        "            candidate = (base / suffix).resolve()\n",
        "            if (candidate / \"cafes.csv\").is_file():\n",
        "                return str(candidate)\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not locate cafes.csv near {anchor}. Ensure the CSV directory is checked in.\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Per-cafe POI metrics\n",
        "\n",
        "# Explains how each cafe aggregates nearby POIs into counts, weights, and distance features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d9c0701e",
      "metadata": {
        "id": "b7f461d1",
        "language": "python"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ROAD_SNAP_TOLERANCE_M' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Per-cafe POI metrics\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_poi_metrics_for_cafes\u001b[39m(\n\u001b[32m      3\u001b[39m     cafes: pd.DataFrame,\n\u001b[32m      4\u001b[39m     poi: pd.DataFrame,\n\u001b[32m      5\u001b[39m     poi_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m      6\u001b[39m     category_weight: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[32m      7\u001b[39m     radius_m: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m1000.0\u001b[39m,\n\u001b[32m      8\u001b[39m     road_network: Optional[\u001b[33m\"\u001b[39m\u001b[33mRoadNetwork\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     snap_tolerance_m: \u001b[38;5;28mfloat\u001b[39m = \u001b[43mROAD_SNAP_TOLERANCE_M\u001b[49m,\n\u001b[32m     10\u001b[39m     decay_scale_m: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m1000.0\u001b[39m,\n\u001b[32m     11\u001b[39m ) -> pd.DataFrame:\n\u001b[32m     12\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute per-cafe aggregated POI metrics.\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m    If `decay_scale_m` is provided, per-POI weights are decayed exponentially\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m    by distance (meters) using the factor exp(-distance / decay_scale_m).\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m    Road-network shortest-path distances are used when `road_network` is\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    available; otherwise a haversine (straight-line) distance is used.\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     18\u001b[39m     latlon = detect_latlon(poi)\n",
            "\u001b[31mNameError\u001b[39m: name 'ROAD_SNAP_TOLERANCE_M' is not defined"
          ]
        }
      ],
      "source": [
        "# Per-cafe POI metrics\n",
        "def compute_poi_metrics_for_cafes(\n",
        "    cafes: pd.DataFrame,\n",
        "    poi: pd.DataFrame,\n",
        "    poi_name: str,\n",
        "    category_weight: float,\n",
        "    radius_m: float = 1000.0,\n",
        "    road_network: Optional[\"RoadNetwork\"] = None,\n",
        "    snap_tolerance_m: float = ROAD_SNAP_TOLERANCE_M,\n",
        "    decay_scale_m: float = 1000.0,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Compute per-cafe aggregated POI metrics.\n",
        "    If `decay_scale_m` is provided, per-POI weights are decayed exponentially\n",
        "    by distance (meters) using the factor exp(-distance / decay_scale_m).\n",
        "    Road-network shortest-path distances are used when `road_network` is\n",
        "    available; otherwise a haversine (straight-line) distance is used.\n",
        "    \"\"\"\n",
        "    latlon = detect_latlon(poi)\n",
        "    if latlon is None:\n",
        "        # no coordinates in POI, return zeros\n",
        "        cafes[f\"{poi_name}_count_1km\"] = 0\n",
        "        cafes[f\"{poi_name}_weight_1km\"] = 0.0\n",
        "        cafes[f\"{poi_name}_min_dist_m\"] = np.nan\n",
        "        return cafes\n",
        "\n",
        "    poi_lat_col, poi_lon_col = latlon\n",
        "    poi_lats = pd.to_numeric(poi[poi_lat_col], errors=\"coerce\").to_numpy(dtype=float)\n",
        "    poi_lons = pd.to_numeric(poi[poi_lon_col], errors=\"coerce\").to_numpy(dtype=float)\n",
        "\n",
        "    weight_col = detect_weight_col(poi)\n",
        "    # if a precomputed weight column exists (created by helper), use it\n",
        "    if \"_computed_weight\" in poi.columns:\n",
        "        poi_weights = pd.to_numeric(poi[\"_computed_weight\"], errors=\"coerce\").fillna(1.0).to_numpy().astype(float)\n",
        "    else:\n",
        "        # gather optional rating/review columns\n",
        "        def _detect_rating_col(df: pd.DataFrame) -> Optional[str]:\n",
        "            for c in RATING_COL_CANDS:\n",
        "                if c in df.columns:\n",
        "                    return c\n",
        "            return None\n",
        "\n",
        "        def _detect_reviews_col(df: pd.DataFrame) -> Optional[str]:\n",
        "            for c in REVIEWS_COL_CANDS:\n",
        "                if c in df.columns:\n",
        "                    return c\n",
        "            return None\n",
        "\n",
        "        rating_col = _detect_rating_col(poi)\n",
        "        reviews_col = _detect_reviews_col(poi)\n",
        "\n",
        "        if weight_col is not None:\n",
        "            raw_vals = pd.to_numeric(poi[weight_col], errors=\"coerce\")\n",
        "            # base score from rank-like column (lower is better)\n",
        "            if \"rank\" in weight_col.lower():\n",
        "                maxr = raw_vals.max(skipna=True)\n",
        "                if pd.isna(maxr) or maxr <= 0:\n",
        "                    maxr = 1.0\n",
        "                fill_val = float(maxr) + 1.0\n",
        "                ranks = raw_vals.fillna(fill_val).replace(0.0, fill_val).astype(float)\n",
        "                inv = 1.0 / (ranks + 1e-9)\n",
        "                base_norm = inv / float(inv.max()) if inv.max() > 0 else pd.Series(np.ones(len(ranks)), index=ranks.index)\n",
        "            else:\n",
        "                # treat column as higher-is-better; normalize by max\n",
        "                v = raw_vals.fillna(0.0).astype(float)\n",
        "                base_norm = v / float(v.max()) if v.max() > 0 else pd.Series(np.zeros(len(v)), index=v.index)\n",
        "\n",
        "            # optional rating and reviews influence\n",
        "            rating_vals = pd.to_numeric(poi[rating_col], errors=\"coerce\") if rating_col is not None else None\n",
        "            reviews_vals = pd.to_numeric(poi[reviews_col], errors=\"coerce\") if reviews_col is not None else None\n",
        "\n",
        "            # Use base/rank and (optionally) reviews only â€” do NOT use rating or weekly-hours\n",
        "            comps = [base_norm]\n",
        "            if reviews_vals is not None:\n",
        "                rv = reviews_vals.fillna(0.0).astype(float)\n",
        "                maskr = rv > 0\n",
        "                if maskr.any():\n",
        "                    # log-scale normalize by max\n",
        "                    norm_rev = pd.Series(0.0, index=rv.index)\n",
        "                    maxlog = float(np.log1p(rv[maskr]).max())\n",
        "                    if maxlog > 0:\n",
        "                        norm_rev[maskr] = np.log1p(rv[maskr]) / maxlog\n",
        "                    comps.append(norm_rev)\n",
        "\n",
        "            # final per-POI weight is mean of available components\n",
        "            stacked = np.vstack([c.to_numpy() for c in comps])\n",
        "            poi_weights = np.nanmean(stacked, axis=0).astype(float)\n",
        "        else:\n",
        "            # no explicit weight column: use reviews only (do NOT use rating or weekly-hours)\n",
        "            reviews_vals = pd.to_numeric(poi[reviews_col], errors=\"coerce\") if reviews_col is not None else None\n",
        "            if reviews_vals is not None:\n",
        "                rv = reviews_vals.fillna(0.0).astype(float)\n",
        "                maskr = rv > 0\n",
        "                norm_rev = pd.Series(0.0, index=rv.index)\n",
        "                if maskr.any():\n",
        "                    maxlog = float(np.log1p(rv[maskr]).max())\n",
        "                    if maxlog > 0:\n",
        "                        norm_rev[maskr] = np.log1p(rv[maskr]) / maxlog\n",
        "                    poi_weights = norm_rev.to_numpy().astype(float)\n",
        "                else:\n",
        "                    poi_weights = np.ones_like(poi_lats, dtype=float)\n",
        "            else:\n",
        "                poi_weights = np.ones_like(poi_lats, dtype=float)\n",
        "\n",
        "    poi_weights = np.asarray(poi_weights, dtype=float)\n",
        "\n",
        "    # Prepare result columns (use suffix based on radius)\n",
        "    try:\n",
        "        suffix = f\"_{int(radius_m/1000)}km\"\n",
        "    except Exception:\n",
        "        suffix = \"_1km\"\n",
        "    counts = []\n",
        "    weight_sums = []\n",
        "    min_dists = []\n",
        "\n",
        "    # detect cafe lat/lon columns\n",
        "    cafe_latlon = detect_latlon(cafes)\n",
        "    if cafe_latlon is None:\n",
        "        raise ValueError(\"Could not detect lat/lon in cafes CSV\")\n",
        "\n",
        "    cafe_lat_col, cafe_lon_col = cafe_latlon\n",
        "    cafe_lats = pd.to_numeric(cafes[cafe_lat_col], errors=\"coerce\").to_numpy(dtype=float)\n",
        "    cafe_lons = pd.to_numeric(cafes[cafe_lon_col], errors=\"coerce\").to_numpy(dtype=float)\n",
        "\n",
        "    use_network = bool(road_network) and getattr(road_network, \"node_count\", 0) > 0\n",
        "    node_to_poi: Dict[int, List[int]] = defaultdict(list)\n",
        "    poi_snap_offsets: List[float] = []\n",
        "    if use_network:\n",
        "        poi_nodes, poi_snap_offsets = road_network.snap_points(poi_lats, poi_lons, max_snap_m=snap_tolerance_m)\n",
        "        for idx, node_id in enumerate(poi_nodes):\n",
        "            if node_id is not None and math.isfinite(poi_snap_offsets[idx]):\n",
        "                node_to_poi[int(node_id)].append(idx)\n",
        "        if not node_to_poi:\n",
        "            use_network = False\n",
        "\n",
        "    def _network_stats(cafe_node: int, cafe_offset: float) -> Optional[Tuple[int, float, float]]:\n",
        "        if not use_network:\n",
        "            return None\n",
        "        lengths = road_network.shortest_paths_from(cafe_node, cutoff=radius_m)\n",
        "        if not lengths:\n",
        "            return None\n",
        "        total_count = 0\n",
        "        total_weight = 0.0\n",
        "        min_dist = None\n",
        "        for node_id, path_dist in lengths.items():\n",
        "            poi_indices = node_to_poi.get(node_id)\n",
        "            if not poi_indices:\n",
        "                continue\n",
        "            for poi_idx in poi_indices:\n",
        "                total_dist = path_dist + cafe_offset + poi_snap_offsets[poi_idx]\n",
        "                if total_dist <= radius_m:\n",
        "                    total_count += 1\n",
        "                    # apply exponential decay to per-POI weight based on path distance (meters)\n",
        "                    try:\n",
        "                        decayed = float(poi_weights[poi_idx]) * math.exp(-(float(total_dist) / float(decay_scale_m)))\n",
        "                    except Exception:\n",
        "                        decayed = float(poi_weights[poi_idx])\n",
        "                    total_weight += decayed\n",
        "                    if min_dist is None or total_dist < min_dist:\n",
        "                        min_dist = total_dist\n",
        "        if total_count == 0 or min_dist is None:\n",
        "            return None\n",
        "        return total_count, total_weight, float(min_dist)\n",
        "\n",
        "    # Iterate cafes and compute distances\n",
        "    for i in range(len(cafes)):\n",
        "        lat = cafe_lats[i]\n",
        "        lon = cafe_lons[i]\n",
        "        if not math.isfinite(lat) or not math.isfinite(lon):\n",
        "            counts.append(0)\n",
        "            weight_sums.append(0.0)\n",
        "            min_dists.append(float(np.nan))\n",
        "            continue\n",
        "        if use_network:\n",
        "            cafe_node, cafe_offset = road_network.snap_point(lat, lon, max_snap_m=snap_tolerance_m)\n",
        "            if cafe_node is not None:\n",
        "                cafe_offset = float(cafe_offset or 0.0)\n",
        "                net_stats = _network_stats(cafe_node, cafe_offset)\n",
        "                if net_stats is not None:\n",
        "                    cnt, wsum, mind = net_stats\n",
        "                    counts.append(int(cnt))\n",
        "                    weight_sums.append(float(wsum))\n",
        "                    min_dists.append(float(mind))\n",
        "                    continue\n",
        "        dists = haversine_m(lat, lon, poi_lats, poi_lons)  # meters\n",
        "        within_mask = dists <= radius_m\n",
        "        counts.append(int(np.count_nonzero(within_mask)))\n",
        "        if np.any(within_mask):\n",
        "            # apply exponential decay to haversine distances as well\n",
        "            try:\n",
        "                ds = dists[within_mask].astype(float)\n",
        "                pws = poi_weights[within_mask].astype(float)\n",
        "                decayed_arr = pws * np.exp(-(ds / float(decay_scale_m)))\n",
        "                weight_sum = float(np.sum(decayed_arr))\n",
        "            except Exception:\n",
        "                weight_sum = float(np.sum(poi_weights[within_mask]))\n",
        "            weight_sums.append(weight_sum)\n",
        "            min_dists.append(float(np.min(dists[within_mask])))\n",
        "        else:\n",
        "            weight_sums.append(0.0)\n",
        "            min_dists.append(float(np.nan))\n",
        "\n",
        "    cafes[f\"{poi_name}_count{suffix}\"] = counts\n",
        "    cafes[f\"{poi_name}_weight{suffix}\"] = weight_sums\n",
        "    cafes[f\"{poi_name}_min_dist_m\"] = min_dists\n",
        "\n",
        "    # Also store category weight so downstream composite score can use it\n",
        "    cafes[f\"{poi_name}_category_weight\"] = category_weight\n",
        "\n",
        "    return cafes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73d33cbc",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
